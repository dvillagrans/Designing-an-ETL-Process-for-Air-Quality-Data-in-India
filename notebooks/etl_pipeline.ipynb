{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0f1a1b",
   "metadata": {},
   "source": [
    "# Configuracion del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debd6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (12.24.0)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-storage-blob) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-storage-blob) (44.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-storage-blob) (4.12.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-storage-blob) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diego\\dataspellprojects\\air-quality-etl\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, when, lit\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a6b9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El contenedor 'landing-zone' no existe. Creándolo...\n",
      "Contenedor 'landing-zone' creado exitosamente.\n",
      "Subiendo city_day.csv a Azure Blob Storage...\n",
      "city_day.csv subido exitosamente.\n",
      "Subiendo city_hour.csv a Azure Blob Storage...\n",
      "city_hour.csv subido exitosamente.\n",
      "Subiendo stations.csv a Azure Blob Storage...\n",
      "stations.csv subido exitosamente.\n",
      "Subiendo station_day.csv a Azure Blob Storage...\n",
      "station_day.csv subido exitosamente.\n",
      "Subiendo station_hour.csv a Azure Blob Storage...\n",
      "station_hour.csv subido exitosamente.\n",
      "Todos los archivos han sido subidos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Configurar conexión a Azure Blob Storage\n",
    "CONNECTION_STRING = \"DefaultEndpointsProtocol=https;AccountName=etlbasesavanzadas;AccountKey=Q43XAlHcXaXgyet05FvwLBntN8EoP2Dx59g8jwmLD+Ox345nFPLAJDanZb5c3M+R0JnKt8Crg7S5+AStYU9JAw==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\n",
    "landing_zone_container = blob_service_client.get_container_client(\"landing-zone\")\n",
    "\n",
    "# Crear el contenedor si no existe\n",
    "if not landing_zone_container.exists():\n",
    "    print(\"El contenedor 'landing-zone' no existe. Creándolo...\")\n",
    "    blob_service_client.create_container(\"landing-zone\")\n",
    "    print(\"Contenedor 'landing-zone' creado exitosamente.\")\n",
    "\n",
    "# Ruta local donde están los archivos\n",
    "local_folder_path = r\"C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\"\n",
    "\n",
    "# Subir cada archivo de la carpeta local al contenedor\n",
    "for file_name in os.listdir(local_folder_path):\n",
    "    file_path = os.path.join(local_folder_path, file_name)\n",
    "    \n",
    "    if os.path.isfile(file_path):  # Asegúrate de que sea un archivo\n",
    "        blob_client = landing_zone_container.get_blob_client(file_name)\n",
    "        print(f\"Subiendo {file_name} a Azure Blob Storage...\")\n",
    "        \n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        \n",
    "        print(f\"{file_name} subido exitosamente.\")\n",
    "\n",
    "print(\"Todos los archivos han sido subidos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4429fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"Air Quality ETL\").getOrCreate()\n",
    "\n",
    "# Definir directorios\n",
    "landing_zone_path = r\"C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\"\n",
    "raw_zone_path = r\"C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\raw-zone\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dad9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\diego\\\\DataspellProjects\\\\air-quality-etl\\\\data\\\\landing-zone\\\\city_day.csv', 'C:\\\\Users\\\\diego\\\\DataspellProjects\\\\air-quality-etl\\\\data\\\\landing-zone\\\\city_hour.csv', 'C:\\\\Users\\\\diego\\\\DataspellProjects\\\\air-quality-etl\\\\data\\\\landing-zone\\\\stations.csv', 'C:\\\\Users\\\\diego\\\\DataspellProjects\\\\air-quality-etl\\\\data\\\\landing-zone\\\\station_day.csv', 'C:\\\\Users\\\\diego\\\\DataspellProjects\\\\air-quality-etl\\\\data\\\\landing-zone\\\\station_hour.csv']\n"
     ]
    }
   ],
   "source": [
    "# Listar todos los archivos CSV en el landing zone\n",
    "file_paths = [os.path.join(landing_zone_path, f) for f in os.listdir(landing_zone_path) if f.endswith('.csv')]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d095e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_imputation_rules(df):\n",
    "    from pyspark.sql.functions import mean, when, col, lit\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.ml.clustering import KMeans\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType\n",
    "\n",
    "    # 1. Imputar PM2.5 con la media\n",
    "    pm25_mean = df.select(mean(col(\"`PM2.5`\"))).collect()[0][0]\n",
    "    df = df.withColumn(\"PM2.5\", when(col(\"`PM2.5`\").isNull(), lit(pm25_mean)).otherwise(col(\"`PM2.5`\")))\n",
    "\n",
    "    # 2. Imputar PM10 con Regresión Lineal usando PM2.5\n",
    "    complete_rows_pm10 = df.filter(col(\"`PM2.5`\").isNotNull() & col(\"`PM10`\").isNotNull())\n",
    "    missing_rows_pm10 = df.filter(col(\"`PM10`\").isNull() & col(\"`PM2.5`\").isNotNull())\n",
    "\n",
    "    if not missing_rows_pm10.rdd.isEmpty():\n",
    "        assembler_pm10 = VectorAssembler(inputCols=[\"PM2.5\"], outputCol=\"features\")\n",
    "        train_data_pm10 = assembler_pm10.transform(complete_rows_pm10).select(\"features\", \"PM10\")\n",
    "        \n",
    "        lr_pm10 = LinearRegression(featuresCol=\"features\", labelCol=\"PM10\")\n",
    "        lr_model_pm10 = lr_pm10.fit(train_data_pm10)\n",
    "        \n",
    "        test_data_pm10 = assembler_pm10.transform(missing_rows_pm10)\n",
    "        predictions_pm10 = lr_model_pm10.transform(test_data_pm10).select(\"prediction\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        for i, row in enumerate(missing_rows_pm10.collect()):\n",
    "            df = df.withColumn(\"PM10\", when(col(\"PM10\").isNull(), lit(predictions_pm10[i])).otherwise(col(\"PM10\")))\n",
    "\n",
    "    # 3. Imputar AQI con Regresión Lineal usando PM2.5 y PM10\n",
    "    complete_rows_aqi = df.filter(col(\"`PM2.5`\").isNotNull() & col(\"`PM10`\").isNotNull() & col(\"`AQI`\").isNotNull())\n",
    "    missing_rows_aqi = df.filter(col(\"`AQI`\").isNull() & col(\"`PM2.5`\").isNotNull() & col(\"`PM10`\").isNotNull())\n",
    "\n",
    "    if not missing_rows_aqi.rdd.isEmpty():\n",
    "        assembler_aqi = VectorAssembler(inputCols=[\"PM2.5\", \"PM10\"], outputCol=\"features\")\n",
    "        train_data_aqi = assembler_aqi.transform(complete_rows_aqi).select(\"features\", \"AQI\")\n",
    "        \n",
    "        lr_aqi = LinearRegression(featuresCol=\"features\", labelCol=\"AQI\")\n",
    "        lr_model_aqi = lr_aqi.fit(train_data_aqi)\n",
    "        \n",
    "        test_data_aqi = assembler_aqi.transform(missing_rows_aqi)\n",
    "        predictions_aqi = lr_model_aqi.transform(test_data_aqi).select(\"prediction\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        for i, row in enumerate(missing_rows_aqi.collect()):\n",
    "            df = df.withColumn(\"AQI\", when(col(\"AQI\").isNull(), lit(predictions_aqi[i])).otherwise(col(\"AQI\")))\n",
    "\n",
    "    # 4. Imputar AQI_Bucket basándose en AQI\n",
    "    def calculate_bucket(aqi):\n",
    "        if aqi <= 50:\n",
    "            return \"Good\"\n",
    "        elif aqi <= 100:\n",
    "            return \"Satisfactory\"\n",
    "        elif aqi <= 200:\n",
    "            return \"Moderate\"\n",
    "        elif aqi <= 300:\n",
    "            return \"Poor\"\n",
    "        elif aqi <= 400:\n",
    "            return \"Very Poor\"\n",
    "        else:\n",
    "            return \"Severe\"\n",
    "\n",
    "    bucket_udf = udf(calculate_bucket, StringType())\n",
    "    df = df.withColumn(\"AQI_Bucket\", when(col(\"AQI_Bucket\").isNull(), bucket_udf(col(\"AQI\"))).otherwise(col(\"AQI_Bucket\")))\n",
    "\n",
    "    # 5. Imputar otros valores con KMeans\n",
    "    columns_for_clustering = [\"PM2.5\", \"PM10\", \"AQI\"]\n",
    "    assembler_cluster = VectorAssembler(inputCols=columns_for_clustering, outputCol=\"features\")\n",
    "    cluster_data = assembler_cluster.transform(df.dropna(subset=columns_for_clustering))\n",
    "    \n",
    "    kmeans = KMeans(k=3, seed=1)\n",
    "    kmeans_model = kmeans.fit(cluster_data)\n",
    "    cluster_centers = kmeans_model.clusterCenters()\n",
    "\n",
    "    # Asignar valores faltantes con centroides del cluster más cercano\n",
    "    df = df.withColumn(\"cluster\", kmeans_model.transform(df).select(\"prediction\"))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d86d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Processing\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca84181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_day.csv\n",
      "Esquema del archivo C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_day.csv:\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- PM2.5: double (nullable = true)\n",
      " |-- PM10: double (nullable = true)\n",
      " |-- NO: double (nullable = true)\n",
      " |-- NO2: double (nullable = true)\n",
      " |-- NOx: double (nullable = true)\n",
      " |-- NH3: double (nullable = true)\n",
      " |-- CO: double (nullable = true)\n",
      " |-- SO2: double (nullable = true)\n",
      " |-- O3: double (nullable = true)\n",
      " |-- Benzene: double (nullable = true)\n",
      " |-- Toluene: double (nullable = true)\n",
      " |-- Xylene: double (nullable = true)\n",
      " |-- AQI: double (nullable = true)\n",
      " |-- AQI_Bucket: string (nullable = true)\n",
      "\n",
      "Error al aplicar reglas de imputación en C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_day.csv: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 114) (DiegoUwU.mshome.net executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n",
      "Procesando archivo: C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_hour.csv\n",
      "Esquema del archivo C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_hour.csv:\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- PM2.5: double (nullable = true)\n",
      " |-- PM10: double (nullable = true)\n",
      " |-- NO: double (nullable = true)\n",
      " |-- NO2: double (nullable = true)\n",
      " |-- NOx: double (nullable = true)\n",
      " |-- NH3: double (nullable = true)\n",
      " |-- CO: double (nullable = true)\n",
      " |-- SO2: double (nullable = true)\n",
      " |-- O3: double (nullable = true)\n",
      " |-- Benzene: double (nullable = true)\n",
      " |-- Toluene: double (nullable = true)\n",
      " |-- Xylene: double (nullable = true)\n",
      " |-- AQI: double (nullable = true)\n",
      " |-- AQI_Bucket: string (nullable = true)\n",
      "\n",
      "Error al aplicar reglas de imputación en C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\city_hour.csv: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 133) (DiegoUwU.mshome.net executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n",
      "Procesando archivo: C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\stations.csv\n",
      "Esquema del archivo C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\stations.csv:\n",
      "root\n",
      " |-- StationId: string (nullable = true)\n",
      " |-- StationName: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n",
      "Error al aplicar reglas de imputación en C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\stations.csv: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `PM2.5` cannot be resolved. Did you mean one of the following? [`City`, `State`, `Status`, `StationId`, `StationName`].;\n",
      "'Aggregate [unresolvedalias(avg('`PM2.5`), Some(org.apache.spark.sql.Column$$Lambda/0x0000022acce9e6c0@1c0245d6))]\n",
      "+- Relation [StationId#1187,StationName#1188,City#1189,State#1190,Status#1191] csv\n",
      "\n",
      "Procesando archivo: C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_day.csv\n",
      "Esquema del archivo C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_day.csv:\n",
      "root\n",
      " |-- StationId: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- PM2.5: double (nullable = true)\n",
      " |-- PM10: double (nullable = true)\n",
      " |-- NO: double (nullable = true)\n",
      " |-- NO2: double (nullable = true)\n",
      " |-- NOx: double (nullable = true)\n",
      " |-- NH3: double (nullable = true)\n",
      " |-- CO: double (nullable = true)\n",
      " |-- SO2: double (nullable = true)\n",
      " |-- O3: double (nullable = true)\n",
      " |-- Benzene: double (nullable = true)\n",
      " |-- Toluene: double (nullable = true)\n",
      " |-- Xylene: double (nullable = true)\n",
      " |-- AQI: double (nullable = true)\n",
      " |-- AQI_Bucket: string (nullable = true)\n",
      "\n",
      "Error al aplicar reglas de imputación en C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_day.csv: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 1 times, most recent failure: Lost task 0.0 in stage 73.0 (TID 144) (DiegoUwU.mshome.net executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n",
      "Procesando archivo: C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_hour.csv\n",
      "Esquema del archivo C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_hour.csv:\n",
      "root\n",
      " |-- StationId: string (nullable = true)\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- PM2.5: double (nullable = true)\n",
      " |-- PM10: double (nullable = true)\n",
      " |-- NO: double (nullable = true)\n",
      " |-- NO2: double (nullable = true)\n",
      " |-- NOx: double (nullable = true)\n",
      " |-- NH3: double (nullable = true)\n",
      " |-- CO: double (nullable = true)\n",
      " |-- SO2: double (nullable = true)\n",
      " |-- O3: double (nullable = true)\n",
      " |-- Benzene: double (nullable = true)\n",
      " |-- Toluene: double (nullable = true)\n",
      " |-- Xylene: double (nullable = true)\n",
      " |-- AQI: double (nullable = true)\n",
      " |-- AQI_Bucket: string (nullable = true)\n",
      "\n",
      "Error al aplicar reglas de imputación en C:\\Users\\diego\\DataspellProjects\\air-quality-etl\\data\\landing-zone\\station_hour.csv: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 79.0 failed 1 times, most recent failure: Lost task 0.0 in stage 79.0 (TID 163) (DiegoUwU.mshome.net executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"Procesando archivo: {file_path}\")\n",
    "\n",
    "    # Leer archivo CSV\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Verificar el esquema del archivo cargado\n",
    "    print(f\"Esquema del archivo {file_path}:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Aplicar reglas de imputación\n",
    "    try:\n",
    "        df = apply_imputation_rules(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al aplicar reglas de imputación en {file_path}: {e}\")\n",
    "        continue  # Saltar al siguiente archivo si ocurre un error\n",
    "\n",
    "    # Guardar archivo procesado en formato Parquet\n",
    "    output_file_path = os.path.join(raw_zone_path, os.path.basename(file_path).replace(\".csv\", \".parquet\"))\n",
    "    try:\n",
    "        df.write.parquet(output_file_path, mode=\"overwrite\")\n",
    "        print(f\"Archivo procesado y guardado en: {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar archivo en formato Parquet: {e}\")\n",
    ".0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc0d764",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns])\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\diego\\DataspellProjects\\air-quality-etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    720\u001b[0m     )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df1d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
